{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPBYmVldndM4"
      },
      "source": [
        "# Lesson 79: Logistic Regression - Heart Disease Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoDdcwUtJ07u"
      },
      "source": [
        "### Teacher-Student Activities\n",
        "\n",
        "In the previous few classes, you learnt how a logistic regression model classifies labels behind the scenes. \n",
        "\n",
        "In this class, we will continue to build a multivariate logistic regression model to predict whether a patient has heart disease. Let's quickly go through the activities covered in the previous classes and begin this class from **Activity 1: Multivariate Logistic Regression** section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAQyK7yfO14I"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrTUGKPJO61D"
      },
      "source": [
        "#### Recap\n",
        "\n",
        "Run the code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmRB05lddS--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1347b60c-74fe-4645-845d-03485b76f5dd"
      },
      "source": [
        "# Import the required modules and load the heart disease dataset. Also, display the first five rows.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "csv_file = 'https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/uci-heart-disease/heart.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "print(\"\\n\", df.head(), \"\\n\", df.info(), \"\\n\")\n",
        "\n",
        "# Print the number of records with and without heart disease\n",
        "print(\"Number of records in each label are\")\n",
        "print(df['target'].value_counts())\n",
        "\n",
        "# Print the percentage of each label\n",
        "print(\"\\nPercentage of records in each label are\")\n",
        "print(df['target'].value_counts() * 100 / df.shape[0])\n",
        "\n",
        "# Split the training and testing data\n",
        "X = df.drop(columns = 'target')\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 303 entries, 0 to 302\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       303 non-null    int64  \n",
            " 1   sex       303 non-null    int64  \n",
            " 2   cp        303 non-null    int64  \n",
            " 3   trestbps  303 non-null    int64  \n",
            " 4   chol      303 non-null    int64  \n",
            " 5   fbs       303 non-null    int64  \n",
            " 6   restecg   303 non-null    int64  \n",
            " 7   thalach   303 non-null    int64  \n",
            " 8   exang     303 non-null    int64  \n",
            " 9   oldpeak   303 non-null    float64\n",
            " 10  slope     303 non-null    int64  \n",
            " 11  ca        303 non-null    int64  \n",
            " 12  thal      303 non-null    int64  \n",
            " 13  target    303 non-null    int64  \n",
            "dtypes: float64(1), int64(13)\n",
            "memory usage: 33.3 KB\n",
            "\n",
            "    age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
            "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
            "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
            "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
            "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
            "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
            "\n",
            "[5 rows x 14 columns] \n",
            " None \n",
            "\n",
            "Number of records in each label are\n",
            "1    165\n",
            "0    138\n",
            "Name: target, dtype: int64\n",
            "\n",
            "Percentage of records in each label are\n",
            "1    54.455446\n",
            "0    45.544554\n",
            "Name: target, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUuB2VRPPuzf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyTyA2t3PpvR"
      },
      "source": [
        "####Activity 1: Multivariate Logistic Regression^\n",
        "\n",
        "Let's include all the features present in the heart disease dataset to build a multivariate logistic regression model using the `sklearn` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IebsK3oUQQWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b2b47b-4b9b-4c0f-8ed9-e0515fbd200f"
      },
      "source": [
        "# S1.1: Create a multivariate logistic regression model. Also, predict the target values for the train set.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train,y_train)\n",
        "model.score(X_train,y_train)\n",
        "# Predict the target values for the train set.\n",
        "pred = model.predict(X_train)\n",
        "print(confusion_matrix(y_train,pred),classification_report(y_train,pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 79  18]\n",
            " [  9 106]]               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.81      0.85        97\n",
            "           1       0.85      0.92      0.89       115\n",
            "\n",
            "    accuracy                           0.87       212\n",
            "   macro avg       0.88      0.87      0.87       212\n",
            "weighted avg       0.87      0.87      0.87       212\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KQW3TOWox4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "849b695f-e371-43e1-c9ea-5b6cbc49990b"
      },
      "source": [
        "# S1.2: Predict the target values for the test set.\n",
        "# Predict the target values for the train set.\n",
        "pred1 = model.predict(X_test)\n",
        "print(confusion_matrix(y_test,pred1),classification_report(y_test,pred1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[32  9]\n",
            " [ 8 42]]               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79        41\n",
            "           1       0.82      0.84      0.83        50\n",
            "\n",
            "    accuracy                           0.81        91\n",
            "   macro avg       0.81      0.81      0.81        91\n",
            "weighted avg       0.81      0.81      0.81        91\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaFmty9RSSJN"
      },
      "source": [
        "As you can see, \n",
        "- The FP and FN values in the confusion matrix are low\n",
        "- The precision and recall values are also good\n",
        "- The f1-score is also greater than **0.7**\n",
        "\n",
        "This clearly shows that the decision boundary accurately separates the labels (or classes) with good accuracy.\n",
        "\n",
        "But this logistic regression model (refer to the object stored in the `lg_clf_1` variable) is created using all the features (or independent variables). It is quite possible that not all features are of imporatance for the classification of the labels in the `target` column. Therefore, we still can improve the model by reducing the number of features to obtain higher f1-scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_JXv6P_V_vN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vppY0xJcg_ld"
      },
      "source": [
        "#### Activity 2: Data Standardisation^^\n",
        "\n",
        "As you must have observed, when the logistic regression is applied we got the following warning message shown below quite a few times:\n",
        "```\n",
        "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
        "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
        "\n",
        "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
        "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "Please also refer to the documentation for alternative solver options:\n",
        "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
        "```\n",
        "\n",
        "The message is displayed because the **Limited-memory Broyden–Fletcher–Goldfarb–Shanno** (or L-BFGS) algorithm used by the `LogisticRegression` class of the `sklearn.linear_model` module to calculate the optimum value of coefficients (betas) for a regularised cost function ran out of memory to store the results of iterations. The L-BFGS algorithm, unlike gradient descent algorithm, is a second-order (uses second derivatives i.e. $\\frac{\\partial^2 J}{\\partial \\beta^2}$ instead of first-order derivatives i.e. $\\frac{\\partial J}{\\partial \\beta}$ ) optimiser stores data of the last few iterations only to save memory. \n",
        "\n",
        "Another reason for the popping-up of the warning message is poorly scaled data. Here are a couple of ways to avoid `ConvergenceWarning` message:\n",
        "\n",
        "1. Increase the number of iterations i.e. set the value of `max_iter` parameter to 100 i.e. `max_iter = 100` in the `LogisticRegression` constructor.\n",
        "\n",
        "2. Scale the data using one of the normalisation methods, say standard normalisation.\n",
        "\n",
        "Therefore, let's create a function `standard_scalar()` to normalise the `X_train` and `X_test` data-frames using standard normalisation method i.e.\n",
        "\n",
        "$$x_{\\text{std}} = \\frac{(x_i - \\mu)}{\\sigma} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwYdDr3oiE4U"
      },
      "source": [
        "# S2.1: Normalise the train and test data-frames using the standard normalisation method.\n",
        "def stand_scal(s):\n",
        "  return (s-s.mean())/s.std()\n",
        "\n",
        "norm_x_train = X_train.apply(stand_scal, axis=0)\n",
        "norm_x_test = X_test.apply(stand_scal, axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKkQFKBki7KL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e89e981-2f5b-4f1d-97e0-7513323299b9"
      },
      "source": [
        "# S2.2: Display descriptive statistics for the normalised values of the features for the test data-frames.\n",
        "print(norm_x_train.describe())\n",
        "print(norm_x_test.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                age           sex  ...            ca          thal\n",
            "count  2.120000e+02  2.120000e+02  ...  2.120000e+02  2.120000e+02\n",
            "mean   1.921943e-16  8.274304e-17  ...  1.152118e-16  1.110223e-16\n",
            "std    1.000000e+00  1.000000e+00  ...  1.000000e+00  1.000000e+00\n",
            "min   -2.757098e+00 -1.391141e+00  ... -6.746937e-01 -3.912465e+00\n",
            "25%   -7.177485e-01 -1.391141e+00  ... -6.746937e-01 -5.475864e-01\n",
            "50%    7.080006e-02  7.154438e-01  ... -6.746937e-01 -5.475864e-01\n",
            "75%    7.233920e-01  7.154438e-01  ...  3.770347e-01  1.134853e+00\n",
            "max    2.463637e+00  7.154438e-01  ...  3.532220e+00  1.134853e+00\n",
            "\n",
            "[8 rows x 13 columns]\n",
            "                age           sex  ...            ca          thal\n",
            "count  9.100000e+01  9.100000e+01  ...  9.100000e+01  9.100000e+01\n",
            "mean  -2.488852e-16 -1.049222e-16  ... -1.220025e-16  2.464451e-16\n",
            "std    1.000000e+00  1.000000e+00  ...  1.000000e+00  1.000000e+00\n",
            "min   -2.301763e+00 -1.661622e+00  ... -8.102615e-01 -3.491486e+00\n",
            "25%   -8.354271e-01 -1.661622e+00  ... -8.102615e-01 -4.364358e-01\n",
            "50%    1.797284e-01  5.952080e-01  ... -8.102615e-01 -4.364358e-01\n",
            "75%    6.309086e-01  5.952080e-01  ...  9.246514e-01  1.091089e+00\n",
            "max    2.435630e+00  5.952080e-01  ...  2.659564e+00  1.091089e+00\n",
            "\n",
            "[8 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Fw8ItUlKQy"
      },
      "source": [
        "As we can observe in the output, the data is normalised because the mean and standard deviation values for each column are 0 and 1 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8t9xTqji7Fv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcnUnBLDWBUm"
      },
      "source": [
        "####Activity 3: Features Selection Using RFE^^^\n",
        "\n",
        "Our next task is to select the relevant features from all the features that contribute to a person having a heart disease. The irrelevant features do not help in increasing the accuracy of a prediction model. Secondly, they also increase the training time of a model. You don't want to have either a very few features or too many of them in your prediction model.\n",
        "\n",
        "So, the question is **how to select features?**\n",
        "\n",
        "One simpler way is trial and error. You can pick **any one feature** at a time, build a prediction model and evaluate it. \n",
        "\n",
        "Similarly, you pick **any two features** at a time, a prediction model and evaluate it. For example\n",
        "- 1, 2 \n",
        "- 1, 3\n",
        "- 1, 4\n",
        "etc.\n",
        "\n",
        "Similarly, you pick **any three features** at a time, a prediction model and evaluate it. For example\n",
        "- 1, 2, 3 \n",
        "- 1, 2, 4\n",
        "- 2, 3, 4 \n",
        "etc.\n",
        "\n",
        "And so on. However, all this is a very time-consuming process to do manually. Instead, you can use the `RFE` (Recursive Feature Elimination) class of the `sklearn.feature_selection` module.It is a  backward feature selection technique and is based on **feature importance**. You have already learnt how to use RFE in the linear regression lesson(s).\n",
        "\n",
        "So let's try to find the optimal number of features required using RFE to build a logistic regression model to predict whether a person has heart disease. Here is the list of steps below that we will follow for this purpose:\n",
        "\n",
        "1. Import the following modules\n",
        "```\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "```\n",
        "\n",
        "2. Create an empty dictionary and store it in a variable called `dict_rfe`.\n",
        "\n",
        "3. Create a `for` loop that iterates through all the columns in normalised training data-frame. Inside the loop: \n",
        "   \n",
        "   - Create an object of `LogisticRegression` class and store it in a variable called `lg_clf_2`.\n",
        "   \n",
        "   - Create an object of `RFE` class and store it in a variable called `rfe`. Inside the `RFE()` constructor, pass the object of logistic regression and the number of features to be selected by RFE as inputs. \n",
        "   \n",
        "   - Call the `fit()` function of the `RFE` class to train a logistic regression model on the train set with `i` number of features where `i` goes from `1` to `len(X_train.columns)`. \n",
        "   \n",
        "   - The `support_` attribute holds rank value(s) of the selected feature(s) where rank `1` denotes the most important feature.\n",
        "   \n",
        "   - Create a list to store the important features in a variable called `rfe_features`.\n",
        "   \n",
        "   - Create a new data-frame having the features selected by RFE store it in a variable called `rfe_X_train`.\n",
        "   \n",
        "   - Create another `LogisticRegression` object, store it in a variable called `lg_clf_3` and build a logistic regression model using the `rfe_X_train` data-frame and `y_train` series.\n",
        "   \n",
        "   - Predict the target values for the normalised test set (containing the feature(s) selected by RFE) by calling the `predict()` function on `lg_clf_3` object.\n",
        "   \n",
        "   - Calculate f1-scores using the function `f1_score()` function of `sklearn.metrics` module that returns a NumPy array containing f1-scores for both the classes. Store the array in a variable called `f1_scores_array`. The **syntax** for the `f1_score()` function is `f1_score(y_true, y_pred, average = None)`\n",
        "     where `y_true` and `y_pred` are the actual and predicted labels respectively, and `average = None` parameter returns the scores for each class.\n",
        "\n",
        "   - Add the number of selected features and corresponding features & f1-scores as key-value pairs in the `dict_rfe` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTJAe6qdSxTu"
      },
      "source": [
        "# S3.1: Create a dictionary containing the different combination of features selected by RFE and their corresponding f1-scores.\n",
        "# Import the libraries\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import f1_score\n",
        "# Create the empty dictionary.\n",
        "d = {}\n",
        "# Create a loop\n",
        "for i in range(1,len(X_train.columns)+1):\n",
        "  lr1 = LogisticRegression()\n",
        "  rfe = RFE(lr1,n_features_to_select=i)\n",
        "  rfe.fit(norm_x_train,y_train)\n",
        "  rfe_ft = list(norm_x_train.columns[rfe.support_])\n",
        "  rfe_x_train = norm_x_train[rfe_ft]\n",
        "  lr2 = LogisticRegression()\n",
        "  lr2.fit(rfe_x_train,y_train)\n",
        "  pred2 = lr2.predict(norm_x_test[rfe_ft])\n",
        "  f1ar = f1_score(y_test,pred2,average= None)\n",
        "  d[i] = {'features':list(rfe_ft),'f1_score':f1ar}\n",
        "  # Build a logistic regression model using the features selected by RFE.\n",
        "  \n",
        "  # Predicting 'y' values only for the test set as generally, they are predicted quite accurately for the train set.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Usuq9UBZwV"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "1. ```\n",
        "   lg_clf_2 = LogisticRegression()\n",
        "   rfe = RFE(lg_clf_2, i)\n",
        "   rfe.fit(norm_X_train, y_train)\n",
        "   ```\n",
        "   part gets the most important features using RFE.\n",
        "\n",
        "2. ```\n",
        "   rfe_features = list(norm_X_train.columns[rfe.support_])\n",
        "   rfe_X_train = norm_X_train[rfe_features]\n",
        "   ```\n",
        "   part creates a new data-frame containing the values of the most important feature(s) selected by RFE.\n",
        "\n",
        "3. ```\n",
        "   lg_clf_3 = LogisticRegression()\n",
        "   lg_clf_3.fit(rfe_X_train, y_train)\n",
        "   ```\n",
        "   part builds a logistic regression model using the most important feature(s) selected by RFE.\n",
        "\n",
        "4. ```\n",
        "   y_test_pred = lg_clf_3.predict(norm_X_test[rfe_features])\n",
        "   ```\n",
        "   part predicts the target values on the test set only as generally a machine learning model performs well on the training set. \n",
        "\n",
        "5. ```\n",
        "   f1_scores_array = f1_score(y_test, y_test_pred, average = None)\n",
        "   ```\n",
        "   part calculates f1-scores\n",
        "\n",
        "6. ```\n",
        "   dict_rfe[i] = {\"features\": list(rfe_features), \"f1_score\": f1_scores_array}\n",
        "   ```\n",
        "   part adds the number of features, features and their corresponding f1-scores as key-value pairs to the dictionary stored in the `dict_rfe` variable.\n",
        "\n",
        "Let's print the dictionary created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx87KQFxSxTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb82b4ef-a742-4d96-fec9-5c12f89de799"
      },
      "source": [
        "# S3.2: Print the dictionary created in the previous exercise.\n",
        "d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {'f1_score': array([0.65789474, 0.75471698]), 'features': ['oldpeak']},\n",
              " 2: {'f1_score': array([0.78378378, 0.85185185]),\n",
              "  'features': ['cp', 'oldpeak']},\n",
              " 3: {'f1_score': array([0.8       , 0.84313725]),\n",
              "  'features': ['cp', 'oldpeak', 'ca']},\n",
              " 4: {'f1_score': array([0.76923077, 0.82692308]),\n",
              "  'features': ['cp', 'oldpeak', 'ca', 'thal']},\n",
              " 5: {'f1_score': array([0.79487179, 0.84615385]),\n",
              "  'features': ['cp', 'exang', 'oldpeak', 'ca', 'thal']},\n",
              " 6: {'f1_score': array([0.8       , 0.84313725]),\n",
              "  'features': ['cp', 'exang', 'oldpeak', 'slope', 'ca', 'thal']},\n",
              " 7: {'f1_score': array([0.78481013, 0.83495146]),\n",
              "  'features': ['sex', 'cp', 'exang', 'oldpeak', 'slope', 'ca', 'thal']},\n",
              " 8: {'f1_score': array([0.79487179, 0.84615385]),\n",
              "  'features': ['sex',\n",
              "   'cp',\n",
              "   'restecg',\n",
              "   'exang',\n",
              "   'oldpeak',\n",
              "   'slope',\n",
              "   'ca',\n",
              "   'thal']},\n",
              " 9: {'f1_score': array([0.82051282, 0.86538462]),\n",
              "  'features': ['sex',\n",
              "   'cp',\n",
              "   'restecg',\n",
              "   'thalach',\n",
              "   'exang',\n",
              "   'oldpeak',\n",
              "   'slope',\n",
              "   'ca',\n",
              "   'thal']},\n",
              " 10: {'f1_score': array([0.82051282, 0.86538462]),\n",
              "  'features': ['sex',\n",
              "   'cp',\n",
              "   'fbs',\n",
              "   'restecg',\n",
              "   'thalach',\n",
              "   'exang',\n",
              "   'oldpeak',\n",
              "   'slope',\n",
              "   'ca',\n",
              "   'thal']},\n",
              " 11: {'f1_score': array([0.82051282, 0.86538462]),\n",
              "  'features': ['sex',\n",
              "   'cp',\n",
              "   'trestbps',\n",
              "   'fbs',\n",
              "   'restecg',\n",
              "   'thalach',\n",
              "   'exang',\n",
              "   'oldpeak',\n",
              "   'slope',\n",
              "   'ca',\n",
              "   'thal']},\n",
              " 12: {'f1_score': array([0.81012658, 0.85436893]),\n",
              "  'features': ['sex',\n",
              "   'cp',\n",
              "   'trestbps',\n",
              "   'chol',\n",
              "   'fbs',\n",
              "   'restecg',\n",
              "   'thalach',\n",
              "   'exang',\n",
              "   'oldpeak',\n",
              "   'slope',\n",
              "   'ca',\n",
              "   'thal']},\n",
              " 13: {'f1_score': array([0.81012658, 0.85436893]),\n",
              "  'features': ['age',\n",
              "   'sex',\n",
              "   'cp',\n",
              "   'trestbps',\n",
              "   'chol',\n",
              "   'fbs',\n",
              "   'restecg',\n",
              "   'thalach',\n",
              "   'exang',\n",
              "   'oldpeak',\n",
              "   'slope',\n",
              "   'ca',\n",
              "   'thal']}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV8-ktoQAXTo"
      },
      "source": [
        "Let's convert the `dict_rfe` dictionary to a Pandas DataFrame using the `from_dict()` function of `pandas` module. Pass `orient = index` parameter to the function to orient the DataFrame index-wise. Otherwise, the keys of the dictionary i.e. (1 through 12) will become columns.\n",
        "\n",
        "Moreover, we need columns having larger width in the data-frame as the columns will contain lists and arrays as their values. To do this you can use the `max_colwidth` attribute.\n",
        "\n",
        "**Syntax:** `pd.options.display.max_colwidth = W`\n",
        "\n",
        "where `W` is the required column width.\n",
        "\n",
        "Let's set the column widths to 100.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGKNSmKZaTck",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "531a1559-b9ed-4769-bb18-377181c7dc8c"
      },
      "source": [
        "# S3.3: Convert the dictionary to the dataframe\n",
        "pd.options.display.max_colwidth = 100\n",
        "df1 = pd.DataFrame.from_dict(d,orient='index')\n",
        "\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7f1b3f8e-24ba-4146-8ab2-cd46c9ad29de\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[oldpeak]</td>\n",
              "      <td>[0.6578947368421052, 0.7547169811320756]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[cp, oldpeak]</td>\n",
              "      <td>[0.7837837837837839, 0.851851851851852]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[cp, oldpeak, ca]</td>\n",
              "      <td>[0.8, 0.8431372549019608]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[cp, oldpeak, ca, thal]</td>\n",
              "      <td>[0.7692307692307694, 0.826923076923077]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[cp, exang, oldpeak, ca, thal]</td>\n",
              "      <td>[0.7948717948717948, 0.8461538461538461]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[cp, exang, oldpeak, slope, ca, thal]</td>\n",
              "      <td>[0.8, 0.8431372549019608]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[sex, cp, exang, oldpeak, slope, ca, thal]</td>\n",
              "      <td>[0.7848101265822786, 0.8349514563106797]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[sex, cp, restecg, exang, oldpeak, slope, ca, thal]</td>\n",
              "      <td>[0.7948717948717948, 0.8461538461538461]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[sex, cp, restecg, thalach, exang, oldpeak, slope, ca, thal]</td>\n",
              "      <td>[0.8205128205128206, 0.8653846153846153]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[sex, cp, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]</td>\n",
              "      <td>[0.8205128205128206, 0.8653846153846153]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[sex, cp, trestbps, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]</td>\n",
              "      <td>[0.8205128205128206, 0.8653846153846153]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]</td>\n",
              "      <td>[0.810126582278481, 0.8543689320388349]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]</td>\n",
              "      <td>[0.810126582278481, 0.8543689320388349]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f1b3f8e-24ba-4146-8ab2-cd46c9ad29de')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f1b3f8e-24ba-4146-8ab2-cd46c9ad29de button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f1b3f8e-24ba-4146-8ab2-cd46c9ad29de');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                  features                                  f1_score\n",
              "1                                                                                [oldpeak]  [0.6578947368421052, 0.7547169811320756]\n",
              "2                                                                            [cp, oldpeak]   [0.7837837837837839, 0.851851851851852]\n",
              "3                                                                        [cp, oldpeak, ca]                 [0.8, 0.8431372549019608]\n",
              "4                                                                  [cp, oldpeak, ca, thal]   [0.7692307692307694, 0.826923076923077]\n",
              "5                                                           [cp, exang, oldpeak, ca, thal]  [0.7948717948717948, 0.8461538461538461]\n",
              "6                                                    [cp, exang, oldpeak, slope, ca, thal]                 [0.8, 0.8431372549019608]\n",
              "7                                               [sex, cp, exang, oldpeak, slope, ca, thal]  [0.7848101265822786, 0.8349514563106797]\n",
              "8                                      [sex, cp, restecg, exang, oldpeak, slope, ca, thal]  [0.7948717948717948, 0.8461538461538461]\n",
              "9                             [sex, cp, restecg, thalach, exang, oldpeak, slope, ca, thal]  [0.8205128205128206, 0.8653846153846153]\n",
              "10                       [sex, cp, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]  [0.8205128205128206, 0.8653846153846153]\n",
              "11             [sex, cp, trestbps, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]  [0.8205128205128206, 0.8653846153846153]\n",
              "12       [sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]   [0.810126582278481, 0.8543689320388349]\n",
              "13  [age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]   [0.810126582278481, 0.8543689320388349]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAxf5qBWwt4j"
      },
      "source": [
        "From the above data-frame, we can see that we get the best f1-scores for both the classes when we have 3 features which are `cp, oldpeak` and `ca`. Beyond this point, the number of features increase but the f1-scores increase only marginally. Hence, it is best to have these many features to build a prediction model to predict whether a patient has heart disease. \n",
        "\n",
        "Let's now rebuild a logistic regression model with the ideal number of features to predict whether a person has a heart disease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEftTX0v6DLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ea1756-4498-4db9-db78-991023db5b33"
      },
      "source": [
        "# S3.4: Logistic Regression with the ideal number of features.\n",
        "lr3 = LogisticRegression()\n",
        "rfe1 = RFE(lr3,n_features_to_select=9)\n",
        "rfe1.fit(norm_x_train,y_train)\n",
        "rfe_ft = norm_x_train.columns[rfe1.support_]\n",
        "print(rfe_ft)\n",
        "x_train_final = norm_x_train[rfe_ft]\n",
        "lr4 = LogisticRegression()\n",
        "lr4.fit(x_train_final,y_train)\n",
        "pred3 = lr4.predict(norm_x_test[rfe_ft])\n",
        "arr = f1_score(y_test,pred3,average= None)\n",
        "print(arr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['sex', 'cp', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca',\n",
            "       'thal'],\n",
            "      dtype='object')\n",
            "[0.82051282 0.86538462]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5FNaiGFwjfC"
      },
      "source": [
        "Let's stop here. In the next class, we will learn more metrics to evaluate a classification-based machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzywToBswi0j"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y6bLjm1zskH"
      },
      "source": [
        "### **Project**\n",
        "You can now attempt the **Capstone Project 18** on your own.\n",
        "\n",
        "**Capstone Project 18**: https://colab.research.google.com/drive/1SJYyeTu9sl43k_kiozYNCndVely1e7Lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EzqkVahz2bf"
      },
      "source": [
        "---"
      ]
    }
  ]
}